x-default: &node
  # Corresponds to the user set in the Dockerfile and the shiny-server
  # user on the server for proper write perms to mounted directories
  user: "${UID:-1003}:0"
  build:
    context: .
    dockerfile: Dockerfile
  volumes:
    - ./drivers:/jdbc:ro
    - ./src:/tmp/src:ro
    - ./config:/tmp/config:ro
    - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
    - ./target:/tmp/target:rw
    - ./logs:/tmp/logs:rw
  secrets:
    - AWS_CREDENTIALS
    - GH_PEM
    - IPTS_PRD_PASSWORD
    - IPTS_TST_PASSWORD
    - IPTS_CONNECTION
    - SPARK_ENV
  networks:
    - sparknet

# These environmental variables get merged into the env vars of containers.
# See: https://docs.docker.com/reference/compose-file/extension/#example-4
x-spark-env: &spark-env
  SPARK_PUBLIC_DNS: 10.129.122.29
  SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
  SPARK_RPC_AUTHENTICATION_ENABLED: no
  SPARK_RPC_ENCRYPTION_ENABLED: no
  SPARK_SSL_ENABLED: no

  AWS_SHARED_CREDENTIALS_FILE: /run/secrets/AWS_CREDENTIALS
  IPTS_CONNECTION_FILE: /run/secrets/IPTS_CONNECTION
  SPARK_ENV_FILE: /run/secrets/SPARK_ENV

services:
  spark-node-master-dev:
    <<: *node
    profiles: ["", "dev"]
    image: ghcr.io/ccao-data/service-spark-iasworld:dev
    container_name: spark-node-master-dev
    hostname: spark-node-master-dev
    restart: no
    environment:
      <<: *spark-env
      SPARK_MODE: master
      SPARK_MASTER_URL: spark://spark-node-master-dev:7077
      SPARK_UI_PORT: 4041
      SPARK_EXECUTOR_MEMORY: 32g
      SPARK_MASTER_WEBUI_PORT: 8082
    ports:
      - 4041:4041
      - 8082:8082

  spark-node-worker-dev:
    <<: *node
    profiles: ["", "dev"]
    image: ghcr.io/ccao-data/service-spark-iasworld:dev
    container_name: spark-node-worker-dev
    restart: no
    environment:
      <<: *spark-env
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-node-master-dev:7077
      SPARK_WORKER_MEMORY: 32g
      SPARK_WORKER_CORES: 16
      SPARK_WORKER_WEBUI_PORT: 9091
    ports:
      - 9091:9091

  spark-node-master-prod:
    <<: *node
    profiles: [prod]
    image: ghcr.io/ccao-data/service-spark-iasworld:latest
    container_name: spark-node-master-prod
    hostname: spark-node-master-prod
    restart: unless-stopped
    environment:
      <<: *spark-env
      SPARK_MODE: master
      SPARK_MASTER_URL: spark://spark-node-master-prod:7077
      SPARK_UI_PORT: 4040
      SPARK_EXECUTOR_MEMORY: 96g
      SPARK_MASTER_WEBUI_PORT: 8080
    ports:
      - 4040:4040
      - 8080:8080

  spark-node-worker-prod:
    <<: *node
    profiles: [prod]
    image: ghcr.io/ccao-data/service-spark-iasworld:latest
    container_name: spark-node-worker-prod
    restart: unless-stopped
    environment:
      <<: *spark-env
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-node-master-prod:7077
      SPARK_WORKER_MEMORY: 96g
      SPARK_WORKER_CORES: 32
      SPARK_WORKER_WEBUI_PORT: 9090
    ports:
      - 9090:9090

# Using a dedicated subnet because the Docker default subnet conflicts
# with some of the CCAO's internal routing
networks:
  sparknet:
    ipam:
      config:
        - subnet: 211.55.0.0/16
    name: sparknet

secrets:
  AWS_CREDENTIALS:
    file: secrets/AWS_CREDENTIALS_FILE
  GH_PEM:
    file: secrets/GH_PEM
  IPTS_CONNECTION:
    file: secrets/IPTS_CONNECTION
  IPTS_PRD_PASSWORD:
    file: secrets/IPTS_PRD_PASSWORD
  IPTS_TST_PASSWORD:
    file: secrets/IPTS_TST_PASSWORD
  SPARK_ENV:
    file: secrets/SPARK_ENV
