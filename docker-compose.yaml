x-default: &node
  # Corresponds to the user set in the Dockerfile and the shiny-server
  # user on the server for proper write perms to mounted directories
  user: "${UID:-1003}:0"
  build:
    context: .
    dockerfile: Dockerfile
  volumes:
    - ./drivers:/jdbc:ro
    - ./src:/tmp/src:ro
    - ./config:/tmp/config:ro
    - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
    - ./target:/tmp/target:rw
    - ./logs:/tmp/logs:rw
  secrets:
    - AWS_CREDENTIALS
    - GH_PEM
    - IPTS_PRD_PASSWORD
    - IPTS_TST_PASSWORD

# These environmental variables get templated/merged into the env vars of
# containers. Usually YAML doesn't let you merge arrays, but Compose allows it:
# https://docs.docker.com/reference/compose-file/extension/#example-4
x-spark-env: &spark-env
  SPARK_MASTER_URL: spark://spark-node-master:7077
  SPARK_PUBLIC_DNS: ${SPARK_PUBLIC_IP}
  SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
  SPARK_RPC_AUTHENTICATION_ENABLED: no
  SPARK_RPC_ENCRYPTION_ENABLED: no
  SPARK_SSL_ENABLED: no

  AWS_SHARED_CREDENTIALS_FILE: /run/secrets/AWS_CREDENTIALS
  AWS_S3_BUCKET:
  AWS_S3_PREFIX:
  GH_APP_ID:

  IPTS_PRD_SERVICE_NAME:
  IPTS_PRD_HOSTNAME:
  IPTS_PRD_USERNAME:
  IPTS_PRD_PORT:
  IPTS_TST_SERVICE_NAME:
  IPTS_TST_HOSTNAME:
  IPTS_TST_USERNAME:
  IPTS_TST_PORT:

services:
  spark-node-master-prod:
    <<: *node
    profiles: ["prod", ""] # empty string here runs when no profile is specified
    image: ${CCAO_REGISTRY_URL}/service-spark-iasworld:latest
    container_name: spark-node-master-prod
    hostname: spark-node-master
    restart: unless-stopped
    environment:
      <<: [*spark-env]
      SPARK_MODE: master
      SPARK_UI_PORT: 4040
      SPARK_EXECUTOR_MEMORY: 96g
      SPARK_MASTER_WEBUI_PORT: 8080
    ports:
      - 4040:4040
      - 8080:8080
    networks:
      - sparknet-prod

  spark-node-worker-prod:
    <<: *node
    profiles: ["prod", ""]
    image: ${CCAO_REGISTRY_URL}/service-spark-iasworld:latest
    container_name: spark-node-worker-prod
    hostname: spark-node-worker
    restart: unless-stopped
    depends_on:
      - spark-node-master-prod
    environment:
      <<: [*spark-env]
      SPARK_MODE: worker
      SPARK_WORKER_MEMORY: 96G
      SPARK_WORKER_CORES: 32
      SPARK_WORKER_WEBUI_PORT: 9090
    ports:
      - 9090:9090
    networks:
      - sparknet-prod

  spark-node-master-dev:
    <<: *node
    profiles: ["dev"]
    image: ${CCAO_REGISTRY_URL}/service-spark-iasworld:dev
    container_name: spark-node-master-dev
    hostname: spark-node-master
    restart: no
    environment:
      <<: [*spark-env]
      SPARK_MODE: master
      SPARK_UI_PORT: 4042
      SPARK_EXECUTOR_MEMORY: 48g
      # 8081 is the worker port by default and binding it will block jobs. So
      # we skip to 8082 instead of using the incremented port of 8081
      SPARK_MASTER_WEBUI_PORT: 8082
    ports:
      - 4042:4042
      - 8082:8082
    networks:
      - sparknet-dev

  spark-node-worker-dev:
    <<: *node
    profiles: ["dev"]
    image: ${CCAO_REGISTRY_URL}/service-spark-iasworld:dev
    container_name: spark-node-worker-dev
    hostname: spark-node-worker
    restart: no
    depends_on:
      - spark-node-master-dev
    environment:
      <<: [*spark-env]
      SPARK_MODE: worker
      SPARK_WORKER_MEMORY: 48G
      SPARK_WORKER_CORES: 14
      SPARK_WORKER_WEBUI_PORT: 9092
    ports:
      - 9092:9092
    networks:
      - sparknet-dev

# Using a dedicated subnet because the Docker default subnet conflicts
# with some of the CCAO's internal routing
networks:
  sparknet-prod:
    ipam:
      config:
        - subnet: 211.55.0.0/16
    name: sparknet-prod
  sparknet-dev:
    ipam:
      config:
        - subnet: 211.54.0.0/16
    name: sparknet-dev

secrets:
  AWS_CREDENTIALS:
    file: secrets/AWS_CREDENTIALS_FILE
  GH_PEM:
    file: secrets/GH_PEM
  IPTS_PRD_PASSWORD:
    file: secrets/IPTS_PRD_PASSWORD
  IPTS_TST_PASSWORD:
    file: secrets/IPTS_TST_PASSWORD
